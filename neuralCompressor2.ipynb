{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1.e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the MNIST Digits dataset. It consists of ~60000 training images of size 28x28.\n",
    "\n",
    "The goal of using this dataset is that everyone can run it on a laptop, without the use of a cloud provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "triain_img_path = \"MNIST_FASHION\\\\train-images-idx3-ubyte\"\n",
    "train_label_path = \"MNIST_FASHION\\\\train-labels-idx1-ubyte\"\n",
    "test_img_path = \"MNIST_FASHION\\\\t10k-images-idx3-ubyte\"\n",
    "test_label_path = \"MNIST_FASHION\\\\t10k-labels-idx1-ubyte\"\n",
    "\n",
    "with open(train_label_path, 'rb') as lbpath:\n",
    "    train_labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "with open(triain_img_path, 'rb') as imgpath:\n",
    "    train_images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(train_labels), 784)\n",
    "\n",
    "\n",
    "with open(test_label_path, 'rb') as lbpath:\n",
    "    test_labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "with open(test_img_path, 'rb') as imgpath:\n",
    "    test_images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(test_labels), 784)\n",
    "    \n",
    "\n",
    "train_file_list = train_images[:4000]\n",
    "valid_file_list = train_images[4000:4500]\n",
    "test_file_list = test_images[:100]\n",
    "\n",
    "img_size = (28, 28)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((img_size)),])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.file_list[index]\n",
    "        image = np.array(image).astype(np.float32)\n",
    "        # image = Image.open(self.file_list[index])\n",
    "        # if self.transform is not None:\n",
    "        #     image = self.transform(image)\n",
    "\n",
    "        \n",
    "        # image = np.array(image).astype(np.float32)\n",
    "        # image = image.flatten()\n",
    "        #print(image)\n",
    "\n",
    "        #return image\n",
    "        return torch.from_numpy(image).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary code for running some parts, e.g., Causal Convolution 1D for the autoregressive model (ARM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal Convolution for ARM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    A causal 1D convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, A=False, **kwargs):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "\n",
    "        # attributes:\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.A = A\n",
    "        \n",
    "        self.padding = (kernel_size - 1) * dilation + A * 1\n",
    "\n",
    "        # module:\n",
    "        self.conv1d = torch.nn.Conv1d(in_channels, out_channels,\n",
    "                                      kernel_size, stride=1,\n",
    "                                      padding=0,\n",
    "                                      dilation=dilation,\n",
    "                                      **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.pad(x, (self.padding, 0))\n",
    "        conv1d_out = self.conv1d(x)\n",
    "        if self.A:\n",
    "            return conv1d_out[:, :, : -1]\n",
    "        else:\n",
    "            return conv1d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantizer is the crucial component of a neural compressor. It consists of a codebook, a vector of floats. It takes a real-valued input and replaces them with the closests values in the codebook.\n",
    "Please note that we use a real-valued codebook, however, in practice, we can implement it using integers. As a result, we use $K$ bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer(nn.Module):\n",
    "    def __init__(self, input_dim, codebook_dim, temp=1.e7):\n",
    "        super(Quantizer, self).__init__()\n",
    "        \n",
    "        #temperature for softmax\n",
    "        self.temp = temp\n",
    "        \n",
    "        # dimensionality of the inputs and the codebook\n",
    "        self.input_dim = input_dim\n",
    "        self.codebook_dim = codebook_dim\n",
    "        \n",
    "        # codebook layer (a codebook)\n",
    "        # - we initialize it uniformly\n",
    "        # - we make it Parameter, namely, it is learnable\n",
    "        self.codebook = nn.Parameter(torch.FloatTensor(1, self.codebook_dim,).uniform_(-1/self.codebook_dim, 1/self.codebook_dim))\n",
    "    \n",
    "    # A function for codebook indices (a one-hot representation) to values in the codebook.\n",
    "    def indices2codebook(self, indices_onehot):\n",
    "        return torch.matmul(indices_onehot, self.codebook.to(indices_onehot.device).t()).squeeze()\n",
    "    \n",
    "    # A function to change integers to a one-hot representation.\n",
    "    def indices_to_onehot(self, inputs_shape, indices):\n",
    "        indices_hard = torch.zeros(inputs_shape[0], inputs_shape[1], self.codebook_dim, device=indices.device)\n",
    "        indices_hard.scatter_(2, indices, 1)\n",
    "        return indices_hard\n",
    "    \n",
    "    # The forward function:\n",
    "    # - First, distances are calculated between input values and codebook values.\n",
    "    # - Second, indices (soft - differentiable, hard - non-differentiable) between the encoded values and the codebook values are calculated.\n",
    "    # - Third, the quantizer returns indices and quantized code (the output of the encoder).\n",
    "    # - Fourth, the decoder maps the quantized code to the obeservable space (i.e., it decodes the code back).\n",
    "    def forward(self, inputs):\n",
    "        # inputs - a matrix of floats, B x M\n",
    "        inputs_shape = inputs.shape\n",
    "        codebook = self.codebook.to(inputs.device)\n",
    "        # repeat inputs\n",
    "        inputs_repeat = inputs.unsqueeze(2).repeat(1, 1, self.codebook_dim)\n",
    "        # calculate distances between input values and the codebook values\n",
    "        distances = torch.exp(-torch.sqrt(torch.pow(inputs_repeat - self.codebook.unsqueeze(1), 2)))\n",
    "        \n",
    "        # indices (hard, i.e., nondiff)\n",
    "        indices = torch.argmax(distances, dim=2).unsqueeze(2)\n",
    "        indices_hard = self.indices_to_onehot(inputs_shape=inputs_shape, indices=indices)\n",
    "        \n",
    "        # indices (soft, i.e., diff)\n",
    "        indices_soft = torch.softmax(self.temp * distances, -1)\n",
    "        \n",
    "        # quantized values: we use soft indices here because it allows backpropagation\n",
    "        quantized = self.indices2codebook(indices_onehot=indices_soft)\n",
    "        \n",
    "        return (indices_soft, indices_hard, quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoder is simply a neural network that takes an image and outputs a corresponding code.\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_net):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = encoder_net\n",
    "\n",
    "    def encode(self, x):\n",
    "        h_e = self.encoder(x)\n",
    "        return h_e\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder is simply a neural network that takes a quantized code and returns an image.\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_net):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = decoder_net\n",
    "\n",
    "    def decode(self, z):\n",
    "        h_d = self.decoder(z)\n",
    "        return h_d\n",
    "\n",
    "    def forward(self, z, x=None):\n",
    "        x_rec = self.decode(z)\n",
    "        return x_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy Coding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy coding is the crucial step in the compression scheme. At this point, we have a quantized code that we want to transmit. In order to send the quantized code, which is typically represented by discerete (non-binary) symbols, it must be translated into a bitstream (a stream o bits).\n",
    "\n",
    "An entropy coder assigns a unique prefix-free code (e.g., unique binary codes like Huffman codes) to each unique symbol that occurs in the input. Two of the most common entropy encoding techniques are Huffman coding and arithmetic coding that require knowing the (estimates of) probabilities of the symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformEntropyCoding(nn.Module):\n",
    "    def __init__(self, code_dim, codebook_dim):\n",
    "        super(UniformEntropyCoding, self).__init__()\n",
    "        self.code_dim = code_dim\n",
    "        self.codebook_dim = codebook_dim\n",
    "        \n",
    "        self.probs = torch.softmax(torch.ones(1, self.code_dim, self.codebook_dim), -1)\n",
    "    \n",
    "    def sample(self, quantizer=None, B=10):\n",
    "        code = torch.zeros(B, self.code_dim, self.codebook_dim)\n",
    "        for b in range(B):\n",
    "            indx = torch.multinomial(torch.softmax(self.probs, -1).squeeze(0), 1).squeeze()\n",
    "            for i in range(self.code_dim):\n",
    "                code[b,i,indx[i]] = 1\n",
    "        \n",
    "        code = quantizer.indices2codebook(code)\n",
    "        return code\n",
    "    \n",
    "    def forward(self, z, x=None):\n",
    "        p = torch.clamp(self.probs, EPS, 1. - EPS)\n",
    "        return -torch.sum(z * torch.log(p), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndependentEntropyCoding(nn.Module):\n",
    "    def __init__(self, code_dim, codebook_dim):\n",
    "        super(IndependentEntropyCoding, self).__init__()\n",
    "        self.code_dim = code_dim\n",
    "        self.codebook_dim = codebook_dim\n",
    "        \n",
    "        self.probs = nn.Parameter(torch.ones(1, self.code_dim, self.codebook_dim))\n",
    "    \n",
    "    def sample(self, quantizer=None, B=10):\n",
    "        code = torch.zeros(B, self.code_dim, self.codebook_dim)\n",
    "        for b in range(B):\n",
    "            indx = torch.multinomial(torch.softmax(self.probs, -1).squeeze(0), 1).squeeze()\n",
    "            for i in range(self.code_dim):\n",
    "                code[b,i,indx[i]] = 1\n",
    "        \n",
    "        code = quantizer.indices2codebook(code)\n",
    "        return code\n",
    "    \n",
    "    def forward(self, z, x=None):\n",
    "        p = torch.clamp(torch.softmax(self.probs, -1), EPS, 1. - EPS)\n",
    "        return -torch.sum(z * torch.log(p), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARMEntropyCoding(nn.Module):\n",
    "    def __init__(self, code_dim, codebook_dim, arm_net):\n",
    "        super(ARMEntropyCoding, self).__init__()\n",
    "        self.code_dim = code_dim\n",
    "        self.codebook_dim = codebook_dim\n",
    "        self.arm_net = arm_net # it takes B x 1 x code_dim and outputs B x codebook_dim x code_dim\n",
    "    \n",
    "    def f(self, x):\n",
    "        device = next(self.parameters()).device\n",
    "        x = x.to(device)\n",
    "        h = self.arm_net(x.unsqueeze(1))\n",
    "        h = h.permute(0, 2, 1)\n",
    "        p = torch.softmax(h, 2)\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    def sample(self, quantizer=None, B=10):\n",
    "        device = next(self.arm_net.parameters()).device\n",
    "        x_new = torch.zeros((B, self.code_dim), device=device)\n",
    "        \n",
    "        for d in range(self.code_dim):\n",
    "            p = self.f(x_new)\n",
    "            indx_d = torch.multinomial(p[:, d, :], num_samples=1)\n",
    "            codebook_value = quantizer.codebook[0, indx_d].squeeze().to(device)\n",
    "            x_new[:, d] = codebook_value\n",
    "        \n",
    "        return x_new\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        p = self.f(x)\n",
    "        return -torch.sum(z * torch.log(p), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full Neural Compressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCompressor(nn.Module):\n",
    "    def __init__(self, encoder, decoder, entropy_coding, quantizer, beta=1., detaching=False):\n",
    "        super(NeuralCompressor, self).__init__()\n",
    "\n",
    "        # we \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.entropy_coding = entropy_coding\n",
    "        self.quantizer = quantizer\n",
    "        \n",
    "        # beta determines how strongly we focus on compression against reconstruction quality\n",
    "        self.beta = beta\n",
    "        \n",
    "        # We can detach inputs to the rate, then we learn rate and distortion separately\n",
    "        self.detaching = detaching\n",
    "\n",
    "    def forward(self, x, reduction='avg'):\n",
    "        device = x.device\n",
    "        # encoding\n",
    "        #-non-quantized values\n",
    "        z = self.encoder(x)\n",
    "        #-quantizing\n",
    "        quantizer_out = self.quantizer(z)\n",
    "        \n",
    "        # decoding\n",
    "        x_rec = self.decoder(quantizer_out[2])\n",
    "        \n",
    "        # Distortion (e.g., MSE)\n",
    "        Distortion = torch.mean(torch.pow(x - x_rec, 2), 1)\n",
    "        \n",
    "        # Rate: we use the entropy coding here\n",
    "        Rate = torch.mean(self.entropy_coding(quantizer_out[0].to(device), quantizer_out[2].to(device)), 1)\n",
    "        \n",
    "        # Objective\n",
    "        objective = Distortion + self.beta * Rate\n",
    "        \n",
    "        if reduction == 'sum':\n",
    "            return objective.sum(), Distortion.sum(), Rate.sum()\n",
    "        else:\n",
    "            return objective.mean(), Distortion.mean(), Rate.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions: training, evaluation, plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    distortion = 0.\n",
    "    rate = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, test_batch in enumerate(test_loader):\n",
    "        test_batch = test_batch.to(device)\n",
    "        loss_t, distortion_t, rate_t = model_best.forward(test_batch, reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "        distortion = distortion + distortion_t.item()\n",
    "        rate = rate + rate_t.item()\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "    distortion = distortion/N\n",
    "    rate = rate / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: objective={loss} (distortion={distortion}, rate={rate})')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, objective val={loss} (distortion={distortion}, rate={rate})')\n",
    "\n",
    "    return loss, distortion, rate\n",
    "\n",
    "def plot_curve(name, nll_val, metric_name='loss'):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.savefig(name + metric_name + '_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    objective_loss_val = []\n",
    "    objective_distortion_val = []\n",
    "    objective_rate_val = []\n",
    "    loss_best = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, batch in enumerate(training_loader):\n",
    "            batch = batch.to(device)\n",
    "            if hasattr(model, 'dequantization'):\n",
    "                if model.dequantization:\n",
    "                    batch = batch + torch.rand(batch.shape)\n",
    "            loss, _, _ = model.forward(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val, distortion_val, rate_val = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        objective_loss_val.append(loss_val)  # save for plotting\n",
    "        objective_distortion_val.append(distortion_val)  # save for plotting\n",
    "        objective_rate_val.append(rate_val)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            loss_best = loss_val\n",
    "        else:\n",
    "            if loss_val < loss_best:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                loss_best = loss_val\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    objective_loss_val = np.asarray(objective_loss_val)\n",
    "    objective_distortion_val = np.asarray(objective_distortion_val)\n",
    "    objective_rate_val = np.asarray(objective_rate_val)\n",
    "\n",
    "    return objective_loss_val, objective_distortion_val, objective_rate_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageDataset(train_file_list, transform)\n",
    "val_data = ImageDataset(valid_file_list, transform)\n",
    "test_data = ImageDataset(test_file_list, transform)\n",
    "\n",
    "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_coding_type = 'arm' # arm or indp or uniform\n",
    "D = 28 * 28   # input dimension\n",
    "C = 32  # code length\n",
    "E = 32 # codebook size (i.e., the number of quantized values)\n",
    "M = 512  # the number of neurons\n",
    "M_kernels = 64 # the number of kernels in causal conv1d layers\n",
    "\n",
    "# beta: how much we weight rate\n",
    "if entropy_coding_type == 'uniform':\n",
    "    beta = 0. \n",
    "else:\n",
    "    beta = 1.\n",
    "\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 1000 # max. number of epochs\n",
    "max_patience = 50 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = 'results/'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "name = 'neural_compressor_' + entropy_coding_type + '_C_' + str(C) + '_E_' + str(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Neural Compressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER\n",
    "encoder_net = nn.Sequential(nn.Linear(D, M*2), nn.BatchNorm1d(M*2), nn.ReLU(),\n",
    "                            nn.Linear(M*2, M), nn.BatchNorm1d(M), nn.ReLU(),\n",
    "                            nn.Linear(M, M//2), nn.BatchNorm1d(M//2), nn.ReLU(),\n",
    "                            nn.Linear(M//2, M//4), nn.BatchNorm1d(M//4), nn.ReLU(),\n",
    "                            nn.Linear(M//4, C))\n",
    "\n",
    "encoder = Encoder(encoder_net=encoder_net)\n",
    "\n",
    "# DECODER\n",
    "decoder_net = nn.Sequential(nn.Linear(C, M//4), nn.BatchNorm1d(M//4), nn.ReLU(),\n",
    "                            nn.Linear(M//4, M//2), nn.BatchNorm1d(M//2), nn.ReLU(),\n",
    "                            nn.Linear(M//2, M), nn.BatchNorm1d(M), nn.ReLU(),\n",
    "                            nn.Linear(M, M*2), nn.BatchNorm1d(M*2), nn.ReLU(),\n",
    "                            nn.Linear(M*2, D))\n",
    "\n",
    "# # ENCODER\n",
    "# encoder_net = nn.Sequential(nn.Linear(D, M*2), nn.BatchNorm1d(M*2), nn.ReLU(),\n",
    "#                             nn.Linear(M*2, M), nn.BatchNorm1d(M), nn.ReLU(),\n",
    "#                             nn.Linear(M, M//2), nn.BatchNorm1d(M//2), nn.ReLU(),\n",
    "#                             nn.Linear(M//2, C))\n",
    "\n",
    "# encoder = Encoder(encoder_net=encoder_net)\n",
    "\n",
    "# # DECODER\n",
    "# decoder_net = nn.Sequential(nn.Linear(C, M//2), nn.BatchNorm1d(M//2), nn.ReLU(),\n",
    "#                             nn.Linear(M//2, M), nn.BatchNorm1d(M), nn.ReLU(),\n",
    "#                             nn.Linear(M, M*2), nn.BatchNorm1d(M*2), nn.ReLU(),\n",
    "#                             nn.Linear(M*2, D))\n",
    "\n",
    "decoder = Decoder(decoder_net=decoder_net)\n",
    "\n",
    "# QUANTIZER\n",
    "quantizer = Quantizer(input_dim=C, codebook_dim=E)\n",
    "\n",
    "# ENTROPY CODING\n",
    "if entropy_coding_type == 'uniform':\n",
    "    entropy_coding = UniformEntropyCoding(code_dim=C, codebook_dim=E)\n",
    "    \n",
    "elif entropy_coding_type == 'indp':\n",
    "    entropy_coding = IndependentEntropyCoding(code_dim=C, codebook_dim=E)\n",
    "\n",
    "elif entropy_coding_type == 'arm':\n",
    "    kernel = 4\n",
    "    arm_net = nn.Sequential(\n",
    "        CausalConv1d(in_channels=1, out_channels=M_kernels, dilation=1, kernel_size=kernel, A=True, bias=True),\n",
    "        nn.LeakyReLU(),\n",
    "        CausalConv1d(in_channels=M_kernels, out_channels=M_kernels, dilation=1, kernel_size=kernel, A=False, bias=True),\n",
    "        nn.LeakyReLU(),\n",
    "        CausalConv1d(in_channels=M_kernels, out_channels=E, dilation=1, kernel_size=kernel, A=False, bias=True))\n",
    "\n",
    "    entropy_coding = ARMEntropyCoding(code_dim=C, codebook_dim=E, arm_net=arm_net).to(device)\n",
    "\n",
    "# MODEL\n",
    "model = NeuralCompressor(encoder=encoder, decoder=decoder, entropy_coding=entropy_coding, quantizer=quantizer, beta=beta)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, objective val=11357.13875 (distortion=11355.890625, rate=1.2478655090332031)\n",
      "saved!\n",
      "Epoch: 1, objective val=7432.9611875 (distortion=7431.7704375, rate=1.190605628967285)\n",
      "saved!\n",
      "Epoch: 2, objective val=5850.452625 (distortion=5849.28, rate=1.172497489929199)\n",
      "saved!\n",
      "Epoch: 3, objective val=5099.25625 (distortion=5098.11203125, rate=1.1442063674926757)\n",
      "saved!\n",
      "Epoch: 4, objective val=4108.3255 (distortion=4107.1809375, rate=1.1446091384887696)\n",
      "saved!\n",
      "Epoch: 5, objective val=3645.33746875 (distortion=3644.23790625, rate=1.0995674438476561)\n",
      "saved!\n",
      "Epoch: 6, objective val=3378.45921875 (distortion=3377.37015625, rate=1.089020980834961)\n",
      "saved!\n",
      "Epoch: 7, objective val=3083.8265625 (distortion=3082.74771875, rate=1.0789500274658204)\n",
      "saved!\n",
      "Epoch: 8, objective val=2954.292625 (distortion=2953.2345, rate=1.0580690383911133)\n",
      "saved!\n",
      "Epoch: 9, objective val=2858.409421875 (distortion=2857.3435625, rate=1.0659012451171874)\n",
      "saved!\n",
      "Epoch: 10, objective val=2836.559390625 (distortion=2835.533609375, rate=1.0258196640014647)\n",
      "saved!\n",
      "Epoch: 11, objective val=2644.778578125 (distortion=2643.739546875, rate=1.0390577087402344)\n",
      "saved!\n",
      "Epoch: 12, objective val=2600.21746875 (distortion=2599.162625, rate=1.0548383865356445)\n",
      "saved!\n",
      "Epoch: 13, objective val=2500.911125 (distortion=2499.8583125, rate=1.0527865142822266)\n",
      "saved!\n",
      "Epoch: 14, objective val=2410.5730625 (distortion=2409.523265625, rate=1.0498517837524415)\n",
      "saved!\n",
      "Epoch: 15, objective val=2377.5780625 (distortion=2376.519015625, rate=1.0590522003173828)\n",
      "saved!\n",
      "Epoch: 16, objective val=2357.2111875 (distortion=2356.18053125, rate=1.0306548385620118)\n",
      "saved!\n",
      "Epoch: 17, objective val=2277.82653125 (distortion=2276.807125, rate=1.0193461837768554)\n",
      "saved!\n",
      "Epoch: 18, objective val=2303.76165625 (distortion=2302.748, rate=1.0136143035888672)\n",
      "Epoch: 19, objective val=2218.30765625 (distortion=2217.29253125, rate=1.015186134338379)\n",
      "saved!\n",
      "Epoch: 20, objective val=2216.5325625 (distortion=2215.509625, rate=1.0229522857666016)\n",
      "saved!\n",
      "Epoch: 21, objective val=2138.51903125 (distortion=2137.50925, rate=1.009813751220703)\n",
      "saved!\n",
      "Epoch: 22, objective val=2103.84834375 (distortion=2102.84265625, rate=1.0056725387573242)\n",
      "saved!\n",
      "Epoch: 23, objective val=2075.9844375 (distortion=2074.977640625, rate=1.006747230529785)\n",
      "saved!\n",
      "Epoch: 24, objective val=2077.461015625 (distortion=2076.449859375, rate=1.011180450439453)\n",
      "Epoch: 25, objective val=2063.101734375 (distortion=2062.09659375, rate=1.0051023330688476)\n",
      "saved!\n",
      "Epoch: 26, objective val=2060.7875625 (distortion=2059.7841875, rate=1.00332479095459)\n",
      "saved!\n",
      "Epoch: 27, objective val=2063.751375 (distortion=2062.753890625, rate=0.9975243682861328)\n",
      "Epoch: 28, objective val=2030.50196875 (distortion=2029.50075, rate=1.0012328796386718)\n",
      "saved!\n",
      "Epoch: 29, objective val=2009.22446875 (distortion=2008.22203125, rate=1.0024416427612304)\n",
      "saved!\n",
      "Epoch: 30, objective val=2005.560046875 (distortion=2004.5578125, rate=1.002239501953125)\n",
      "saved!\n",
      "Epoch: 31, objective val=2106.645265625 (distortion=2105.63971875, rate=1.0054941940307618)\n",
      "Epoch: 32, objective val=2078.134484375 (distortion=2077.13959375, rate=0.9949534149169922)\n",
      "Epoch: 33, objective val=2091.453125 (distortion=2090.447546875, rate=1.0055229721069336)\n",
      "Epoch: 34, objective val=2042.532953125 (distortion=2041.54446875, rate=0.9884642486572266)\n",
      "Epoch: 35, objective val=2026.770640625 (distortion=2025.78103125, rate=0.9895866317749024)\n",
      "Epoch: 36, objective val=2017.962375 (distortion=2016.960328125, rate=1.00203751373291)\n",
      "Epoch: 37, objective val=2064.3405625 (distortion=2063.35209375, rate=0.9885043640136719)\n",
      "Epoch: 38, objective val=2018.717125 (distortion=2017.740984375, rate=0.9761290740966797)\n",
      "Epoch: 39, objective val=2006.659453125 (distortion=2005.695765625, rate=0.9636833953857422)\n",
      "Epoch: 40, objective val=2022.584484375 (distortion=2021.607421875, rate=0.9770763854980469)\n",
      "Epoch: 41, objective val=1983.98534375 (distortion=1983.028921875, rate=0.9563605499267578)\n",
      "saved!\n",
      "Epoch: 42, objective val=1963.54240625 (distortion=1962.59534375, rate=0.9470751647949218)\n",
      "saved!\n",
      "Epoch: 43, objective val=1959.007234375 (distortion=1958.0625, rate=0.944772590637207)\n",
      "saved!\n",
      "Epoch: 44, objective val=1945.558140625 (distortion=1944.606328125, rate=0.9517749557495118)\n",
      "saved!\n",
      "Epoch: 45, objective val=1945.71859375 (distortion=1944.773171875, rate=0.9454368515014648)\n",
      "Epoch: 46, objective val=1919.6660625 (distortion=1918.707890625, rate=0.9582397918701172)\n",
      "saved!\n",
      "Epoch: 47, objective val=1922.927109375 (distortion=1921.990046875, rate=0.9370518569946289)\n",
      "Epoch: 48, objective val=1921.57540625 (distortion=1920.64109375, rate=0.9343149337768555)\n",
      "Epoch: 49, objective val=2062.68125 (distortion=2061.755921875, rate=0.9252886810302734)\n",
      "Epoch: 50, objective val=1987.701703125 (distortion=1986.767671875, rate=0.9339457702636719)\n",
      "Epoch: 51, objective val=2030.595171875 (distortion=2029.61509375, rate=0.9801162185668946)\n",
      "Epoch: 52, objective val=2000.332 (distortion=1999.3605625, rate=0.9714383697509765)\n",
      "Epoch: 53, objective val=1978.77409375 (distortion=1977.811, rate=0.9631179733276367)\n",
      "Epoch: 54, objective val=1953.795515625 (distortion=1952.843984375, rate=0.9515002136230468)\n",
      "Epoch: 55, objective val=1953.0286875 (distortion=1952.063640625, rate=0.9650777359008789)\n",
      "Epoch: 56, objective val=1947.00921875 (distortion=1946.0416875, rate=0.9675021743774415)\n",
      "Epoch: 57, objective val=1943.961984375 (distortion=1943.003984375, rate=0.9579574356079101)\n",
      "Epoch: 58, objective val=1928.179453125 (distortion=1927.227953125, rate=0.95151611328125)\n",
      "Epoch: 59, objective val=1927.642140625 (distortion=1926.697234375, rate=0.9448256225585937)\n",
      "Epoch: 60, objective val=1921.31959375 (distortion=1920.372953125, rate=0.9466759185791016)\n",
      "Epoch: 61, objective val=1915.30925 (distortion=1914.367125, rate=0.942131591796875)\n",
      "saved!\n",
      "Epoch: 62, objective val=1912.25928125 (distortion=1911.322078125, rate=0.9372232818603515)\n",
      "saved!\n",
      "Epoch: 63, objective val=1913.368546875 (distortion=1912.43084375, rate=0.9376557312011718)\n",
      "Epoch: 64, objective val=1901.7675 (distortion=1900.833859375, rate=0.9336391067504883)\n",
      "saved!\n",
      "Epoch: 65, objective val=1909.831609375 (distortion=1908.903515625, rate=0.928077163696289)\n",
      "Epoch: 66, objective val=1898.074390625 (distortion=1897.147953125, rate=0.9263893356323242)\n",
      "saved!\n",
      "Epoch: 67, objective val=1893.605671875 (distortion=1892.672046875, rate=0.9335811004638672)\n",
      "saved!\n",
      "Epoch: 68, objective val=1883.549046875 (distortion=1882.623015625, rate=0.9260019836425781)\n",
      "saved!\n",
      "Epoch: 69, objective val=1998.384796875 (distortion=1997.44140625, rate=0.9434084320068359)\n",
      "Epoch: 70, objective val=1947.082015625 (distortion=1946.1463125, rate=0.9356937103271484)\n",
      "Epoch: 71, objective val=1918.81778125 (distortion=1917.883578125, rate=0.9342348556518555)\n",
      "Epoch: 72, objective val=1930.916828125 (distortion=1929.992453125, rate=0.9243837966918945)\n",
      "Epoch: 73, objective val=1930.77946875 (distortion=1929.8523125, rate=0.9271685104370118)\n",
      "Epoch: 74, objective val=1911.412296875 (distortion=1910.49528125, rate=0.9170055160522461)\n",
      "Epoch: 75, objective val=1896.201703125 (distortion=1895.289890625, rate=0.9118336791992188)\n",
      "Epoch: 76, objective val=1903.80221875 (distortion=1902.878828125, rate=0.9234509811401367)\n",
      "Epoch: 77, objective val=1933.29453125 (distortion=1932.37653125, rate=0.918065284729004)\n",
      "Epoch: 78, objective val=2028.221515625 (distortion=2027.308484375, rate=0.912962028503418)\n",
      "Epoch: 79, objective val=1973.9064375 (distortion=1972.98609375, rate=0.9202754745483398)\n",
      "Epoch: 80, objective val=1919.03 (distortion=1918.122515625, rate=0.9075303802490234)\n",
      "Epoch: 81, objective val=1907.711078125 (distortion=1906.80740625, rate=0.903696388244629)\n",
      "Epoch: 82, objective val=1896.364921875 (distortion=1895.46890625, rate=0.8959351272583008)\n",
      "Epoch: 83, objective val=1906.1571875 (distortion=1905.2579375, rate=0.8992002868652343)\n",
      "Epoch: 84, objective val=1881.017796875 (distortion=1880.12784375, rate=0.8898968353271485)\n",
      "saved!\n",
      "Epoch: 85, objective val=1903.671359375 (distortion=1902.78175, rate=0.8895530853271484)\n",
      "Epoch: 86, objective val=1897.395796875 (distortion=1896.495515625, rate=0.9003534851074219)\n",
      "Epoch: 87, objective val=1933.1054375 (distortion=1932.178953125, rate=0.9264730529785157)\n",
      "Epoch: 88, objective val=1899.126625 (distortion=1898.223828125, rate=0.9027683715820313)\n",
      "Epoch: 89, objective val=1905.134703125 (distortion=1904.223546875, rate=0.9111489639282226)\n",
      "Epoch: 90, objective val=1902.838125 (distortion=1901.937015625, rate=0.9010954360961914)\n",
      "Epoch: 91, objective val=1868.409203125 (distortion=1867.49775, rate=0.9114239883422851)\n",
      "saved!\n",
      "Epoch: 92, objective val=1879.790109375 (distortion=1878.89609375, rate=0.8940228576660156)\n",
      "Epoch: 93, objective val=1866.638 (distortion=1865.7471875, rate=0.8907631225585938)\n",
      "saved!\n",
      "Epoch: 94, objective val=1861.51475 (distortion=1860.62675, rate=0.8880165557861328)\n",
      "saved!\n",
      "Epoch: 95, objective val=1868.778203125 (distortion=1867.89103125, rate=0.8871982574462891)\n",
      "Epoch: 96, objective val=1865.413234375 (distortion=1864.527421875, rate=0.8858048934936523)\n",
      "Epoch: 97, objective val=1863.107671875 (distortion=1862.228671875, rate=0.8789997863769531)\n",
      "Epoch: 98, objective val=1891.595546875 (distortion=1890.665046875, rate=0.9305147476196289)\n",
      "Epoch: 99, objective val=1908.9329375 (distortion=1908.0291875, rate=0.9038031539916992)\n",
      "Epoch: 100, objective val=1873.08484375 (distortion=1872.179625, rate=0.9052734146118164)\n",
      "Epoch: 101, objective val=1868.892078125 (distortion=1867.98175, rate=0.9103133926391601)\n",
      "Epoch: 102, objective val=1851.429734375 (distortion=1850.52446875, rate=0.9053109817504883)\n",
      "saved!\n",
      "Epoch: 103, objective val=1858.214890625 (distortion=1857.318609375, rate=0.8962682800292969)\n",
      "Epoch: 104, objective val=1890.52953125 (distortion=1889.619734375, rate=0.9097893447875977)\n",
      "Epoch: 105, objective val=1867.575328125 (distortion=1866.68575, rate=0.8895906677246094)\n",
      "Epoch: 106, objective val=1870.240625 (distortion=1869.3250625, rate=0.9155768814086914)\n",
      "Epoch: 107, objective val=1861.4940625 (distortion=1860.580953125, rate=0.9130993957519531)\n",
      "Epoch: 108, objective val=1913.6473125 (distortion=1912.752796875, rate=0.8945314102172851)\n",
      "Epoch: 109, objective val=1875.96628125 (distortion=1875.0691875, rate=0.8970996246337891)\n",
      "Epoch: 110, objective val=1858.01071875 (distortion=1857.122015625, rate=0.8886493377685547)\n",
      "Epoch: 111, objective val=1853.666875 (distortion=1852.79484375, rate=0.8720476531982422)\n",
      "Epoch: 112, objective val=1849.52428125 (distortion=1848.651703125, rate=0.8725789031982422)\n",
      "saved!\n",
      "Epoch: 113, objective val=2006.504328125 (distortion=2005.60265625, rate=0.9016380615234375)\n",
      "Epoch: 114, objective val=1957.36103125 (distortion=1956.451234375, rate=0.9098435592651367)\n",
      "Epoch: 115, objective val=1985.085453125 (distortion=1984.19853125, rate=0.8869183502197265)\n",
      "Epoch: 116, objective val=1907.181453125 (distortion=1906.310953125, rate=0.87047021484375)\n",
      "Epoch: 117, objective val=1876.57690625 (distortion=1875.687265625, rate=0.889673698425293)\n",
      "Epoch: 118, objective val=1958.295265625 (distortion=1957.382296875, rate=0.9130293426513671)\n",
      "Epoch: 119, objective val=1921.47896875 (distortion=1920.57340625, rate=0.9055077056884766)\n",
      "Epoch: 120, objective val=1896.1705 (distortion=1895.265921875, rate=0.904590461730957)\n",
      "Epoch: 121, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 122, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 123, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 124, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 125, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 126, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 127, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 128, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 129, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 130, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 131, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 132, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 133, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 134, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 135, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 136, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 137, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 138, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 139, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 140, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 141, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 142, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 143, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 144, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 145, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 146, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 147, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 148, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 149, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 150, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 151, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 152, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 153, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 154, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 155, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 156, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 157, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 158, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 159, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 160, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 161, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 162, objective val=nan (distortion=nan, rate=nan)\n",
      "Epoch: 163, objective val=nan (distortion=nan, rate=nan)\n"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "objective_loss_val, objective_distortion_val, objective_rate_val = training(name=result_dir + name, max_patience=max_patience, num_epochs=num_epochs, model=model, \n",
    "                   optimizer=optimizer,\n",
    "                   training_loader=training_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL LOSS: objective=1709.7762109375 (distortion=1708.91203125, rate=0.8641237449645996)\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_distortion, test_rate = evaluation(name=result_dir + name, test_loader=test_loader)\n",
    "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
    "f.write(str(test_loss) + ', ' + str(test_distortion) + ', ' + str(test_rate))\n",
    "f.close()\n",
    "\n",
    "plot_curve(result_dir + name + '_objective_', objective_loss_val, metric_name='objective')\n",
    "plot_curve(result_dir + name + '_distortion_', objective_distortion_val, metric_name='distortion')\n",
    "plot_curve(result_dir + name + '_rate_', objective_rate_val, metric_name='rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we visualize samples and reconstructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m IMG_IDs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m99\u001b[39m, \u001b[38;5;241m19\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# samples\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m z_sampled \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentropy_coding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m x_sampled \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder(z_sampled)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# reconstructions\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[90], line 23\u001b[0m, in \u001b[0;36mARMEntropyCoding.sample\u001b[1;34m(self, quantizer, B)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode_dim):\n\u001b[0;32m     22\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(x_new)\n\u001b[1;32m---> 23\u001b[0m     indx_d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     codebook_value \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mcodebook[\u001b[38;5;241m0\u001b[39m, indx_d]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m     x_new[:, d] \u001b[38;5;241m=\u001b[39m codebook_value\n",
      "\u001b[1;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "# We specifies ids of images from the test set.\n",
    "#IMG_IDs = [13, 36, 60, 80]\n",
    "#IMG_IDs = [33, 66, 50, 30]\n",
    "#IMG_IDs = [1, 2, 3, 4]\n",
    "#IMG_IDs = [91, 31, 41, 73] #best reconstruction\n",
    "#IMG_IDs = [92, 93, 94, 95]\n",
    "#IMG_IDs = [51, 52, 53, 54, 99]\n",
    "#IMG_IDs = [55, 56, 57, 58, 59]\n",
    "#IMG_IDs = [61, 62, 63, 64, 65]\n",
    "#IMG_IDs = [67, 69, 70, 71, 72]\n",
    "IMG_IDs = np.random.randint(0, 99, 19)\n",
    "\n",
    "# samples\n",
    "z_sampled = model.entropy_coding.sample(quantizer=model.quantizer, B=9)\n",
    "x_sampled = model.decoder(z_sampled)\n",
    "\n",
    "# reconstructions\n",
    "x_real = torch.stack([test_data.__getitem__(x) for x in IMG_IDs]).to(device)\n",
    "z_encoded = model.encoder(x_real)\n",
    "x_rec = model.decoder(model.quantizer(z_encoded)[-1])\n",
    "\n",
    "# plotting\n",
    "fig, axs = plt.subplots(10, 2, figsize=(10, 10))\n",
    "for i in range(len(IMG_IDs)):\n",
    "    # Original image\n",
    "    original_img = x_real[i].cpu().reshape(28,28).detach().numpy()\n",
    "    axs[i,0].imshow(original_img, cmap ='gray')\n",
    "    original_size = original_img.nbytes\n",
    "    axs[i,0].set_title(f'Original (Size: {original_size} bytes)')\n",
    "    axs[i,0].axis('off')\n",
    "\n",
    "    # Encoded representation\n",
    "    encoded_size = z_encoded[i].cpu().numel() * z_encoded[i].element_size()  # Size in bytes\n",
    "    compression_ratio = original_size / encoded_size\n",
    "    \n",
    "    # Reconstructed image (for visualization)\n",
    "    reconstructed_img = x_rec[i].cpu().reshape(28,28).detach().numpy()\n",
    "    axs[i,1].imshow(reconstructed_img, cmap='gray')\n",
    "    axs[i,1].set_title(f'Reconstruction\\nEncoded size: {encoded_size} bytes\\nCompression ratio: {compression_ratio:.2f}x')\n",
    "    axs[i,1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(result_dir + name + 'recon_sample.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
